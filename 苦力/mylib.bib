@article{akritasApplicationsSingularvalueDecomposition2004,
  title = {Applications of Singular-Value Decomposition ({{SVD}})},
  author = {Akritas, Alkiviadis G. and Malaschonok, Gennadi I.},
  year = {2004},
  month = sep,
  journal = {Mathematics and Computers in Simulation},
  series = {Applications of {{Computer Algebra}} in {{Science}}, {{Engineering}}, {{Simulation}} and {{Special Software}}},
  volume = {67},
  number = {1},
  pages = {15--31},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2004.05.005},
  urldate = {2024-11-15},
  abstract = {Let A be an m{\texttimes}n matrix with m{$\geq$}n. Then one form of the singular-value decomposition of A is A=UT{$\Sigma$}V,where U and V are orthogonal and {$\Sigma$} is square diagonal. That is, UUT=Irank(A), VVT=Irank(A), U is rank(A){\texttimes}m, V is rank(A){\texttimes}n and {$\Sigma$}={$\sigma$}10{$\cdots$}000{$\sigma$}2{$\cdots$}00{$\vdots\vdots\ddots\vdots\vdots$}00{$\cdots\sigma$}rank(A)-1000{$\cdots$}0{$\sigma$}rank(A)is a rank(A){\texttimes}rank(A) diagonal matrix. In addition {$\sigma$}1{$\geq\sigma$}2{$\geq\cdots\geq\sigma$}rank(A){$>$}0. The {$\sigma$}i's are called the singular values of A and their number is equal to the rank of A. The ratio {$\sigma$}1/{$\sigma$}rank(A) can be regarded as a condition number of the matrix A. It is easily verified that the singular-value decomposition can be also written as A=UT{$\Sigma$}V={$\sum$}i=1rank(A){$\sigma$}iuiTvi.The matrix uiTvi is the outer product of the i-th row of U with the corresponding row of V. Note that each of these matrices can be stored using only m+n locations rather than mn locations. Using both forms presented above---and following Jerry Uhl's beautiful approach in the Calculus and Mathematica book series [Matrices, Geometry \& Mathematica, Math Everywhere Inc., 1999]---we show how SVD can be used as a tool for teaching Linear Algebra geometrically, and then apply it in solving least-squares problems and in data compression. In this paper we used the Computer Algebra system Mathematica to present a purely numerical problem. In general, the use of Computer Algebra systems has greatly influenced the teaching of mathematics, allowing students to concentrate on the main ideas and to visualize them.},
  langid = {american},
  keywords = {,Applications,Singular-value decompositions,svd},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\VZJ4WETQ\\Akritas和Malaschonok - 2004 - Applications of singular-value decomposition (SVD).pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LYFHZUA3\\S037847540400151X.html}
}

@misc{ApplicationsSingularvalueDecomposition,
  title = {Applications of Singular-Value Decomposition ({{SVD}}) - {{ScienceDirect}}},
  urldate = {2024-11-15},
  howpublished = {https://www.sciencedirect.com/science/article/abs/pii/S037847540400151X},
  keywords = {,svd},
  file = {C:\Users\guaoxiang\Zotero\storage\WQ723ZBE\S037847540400151X.html}
}

@article{behley3DLiDARbasedSemantic2021,
  title = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences: {{The SemanticKITTI Dataset}}},
  shorttitle = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences},
  author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Gall, J{\"u}rgen and Stachniss, Cyrill},
  year = {2021},
  month = aug,
  journal = {The International Journal of Robotics Research},
  volume = {40},
  number = {8-9},
  pages = {959--967},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783649211006735},
  urldate = {2024-10-13},
  abstract = {A holistic semantic scene understanding exploiting all available sensor modalities is a core capability to master selfdriving in complex everyday traffic. To this end, we present the SemanticKITTI dataset that provides point-wise semantic annotations of Velodyne HDL-64E point clouds of the KITTI Odometry Benchmark. Together with the data, we also published three benchmark tasks for semantic scene understanding covering different aspects of semantic scene understanding: (1) semantic segmentation for point-wise classification using single or multiple point clouds as input, (2) semantic scene completion for predictive reasoning on the semantics and occluded regions, and (3) panoptic segmentation combining point-wise classification and assigning individual instance identities to separate objects of the same class. In this article, we provide details on our dataset showing an unprecedented number of fully annotated point cloud sequences, more information on our labeling process to efficiently annotate such a vast amount of point clouds, and lessons learned in this process. The dataset and resources are available at http://www.semantic-kitti.org.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\JPV7MRAC\Behley 等 - 2021 - Towards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences The SemanticKITTI D.pdf}
}

@article{blackP0VisionLanguageActionFlow,
  title = {{$\Pi$}0: {{A Vision-Language-Action Flow Model}} for {{General Robot Control}}},
  author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and {Li-Bell}, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\VGNCLN49\Black 等 - π0 A Vision-Language-Action Flow Model for General Robot Control.pdf}
}

@misc{bochkovskiiDepthProSharp2024,
  title = {Depth {{Pro}}: {{Sharp Monocular Metric Depth}} in {{Less Than}} a {{Second}}},
  shorttitle = {Depth {{Pro}}},
  author = {Bochkovskii, Aleksei and Delaunoy, Ama{\"e}l and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  year = {2024},
  month = oct,
  number = {arXiv:2410.02073},
  eprint = {2410.02073},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02073},
  urldate = {2024-10-21},
  abstract = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  archiveprefix = {arXiv},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\KRYBNGSD\\Bochkovskii 等 - 2024 - Depth Pro Sharp Monocular Metric Depth in Less Than a Second.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\W2RSKNNQ\\2410.html}
}

@article{CaiJiYuSTM32DeHongWaiChuMoPingDeYanJiuYuShiXian2022,
  title = {{基于STM32的红外触摸屏的研究与实现}},
  author = {蔡, 昌勇 and 朱, 静},
  year = {2022},
  journal = {电脑知识与技术},
  volume = {18},
  number = {14},
  pages = {83--85},
  issn = {1009-3044},
  doi = {10.14004/j.cnki.ckt.2022.0840},
  urldate = {2024-11-01},
  abstract = {交互式液晶面板尺寸越来越大，传统的触摸屏如电阻式触摸屏存在寿命较短、电容式触摸屏价格高的缺点。针对以上问题，本文设计了一种基于STM32的红外触摸屏，通过逐一扫描安装在液晶屏下面及右面的红外发光二极管、安装在液晶屏上面及左面的红外接收二极管，将红外接收二极管输出电压通过STM32内部的ADC转换为数字量，通过数据滤波处理，计算出触摸操作点的坐标，并将坐标值通过USB接口上传至Windows操作系统，实现触摸操作响应。},
  langid = {chinese},
  keywords = {,STM32},
  file = {C:\Users\guaoxiang\Zotero\storage\26JWH6LZ\蔡和朱 - 2022 - 基于STM32的红外触摸屏的研究与实现.pdf}
}

@misc{chenUrdformerPipelineConstructing2024,
  title = {Urdformer: {{A Pipeline}} for {{Constructing Articulated Simulation Environments}} from {{Real-World Images}}},
  shorttitle = {Urdformer},
  author = {Chen, Zoey and Walsman, Aaron and Memmel, Marius and Mo, Kaichun and Fang, Alex and Vemuri, Karthikeya and Wu, Alan and Fox, Dieter and Gupta, Abhishek},
  year = {2024},
  month = may,
  number = {arXiv:2405.11656},
  eprint = {2405.11656},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Constructing simulation scenes that are both visually and physically realistic is a problem of practical interest in domains ranging from robotics to computer vision. This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems. However, building simulation models is often still done by hand. A graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties. While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes, complete with 'natural' kinematic and dynamic structures. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets. To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models. We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments and an integrated system for training robust robotic control policies in the resulting environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\E8GI9TQA\Chen 等 - 2024 - URDFormer A Pipeline for Constructing Articulated Simulation Environments from Real-World Images.pdf}
}

@article{davisonMonoSLAMRealTimeSingle2007,
  title = {{{MonoSLAM}}: {{Real-Time Single Camera SLAM}}},
  shorttitle = {{{MonoSLAM}}},
  author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {6},
  pages = {1052--1067},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2007.1049},
  urldate = {2024-11-06},
  abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the ``pure vision'' domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\88FCC8FE\Davison 等 - 2007 - MonoSLAM Real-Time Single Camera SLAM.pdf}
}

@misc{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03378},
  eprint = {2303.03378},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pretrained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AAEWE6UQ\Driess 等 - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf}
}

@misc{firooziFoundationModelsRobotics2023,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6EADR4YW\\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\C7KSUGVZ\\2312.html}
}

@misc{firooziFoundationModelsRobotics2023a,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper1 can be found here.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\97V5HWLE\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf}
}

@article{gorokhovatskyiSearchVisualObjects2023,
  title = {Search for {{Visual Objects}} by {{Request}} in the {{Form}} of a {{Cluster Representation}} for the {{Structural Image Description}}},
  author = {Gorokhovatskyi, Volodymyr and Tvoroshenko, Iryna and Kobylin, Oleg and Vlasenko, Nataliia},
  year = {2023},
  month = may,
  journal = {Advances in Electrical and Electronic Engineering},
  volume = {21},
  number = {1},
  pages = {19--27},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v21i1.4661},
  urldate = {2024-11-07},
  abstract = {The key task of computer vision is the recognition of visual objects in the analysed image. This paper proposes a method of searching for objects in an image, based on the identification of a cluster representation of the query descriptions and the current image of the window with the calculation of the relevance measure. The implementation of a cluster representation significantly increases the speed of identification or classification of visual objects while maintaining a sufficient level of accuracy. Based on the development of models for the analysis and processing of a set of descriptors of keypoints, we have obtained an effective method for the identification of visual objects. A comparative experiment with the traditional method has been conducted, where a linear search for the nearest descriptor was implemented for identification without using a cluster representation of the description. In the experiment, a speed gain for the developed method has been obtained in comparison with the traditional one by approximately 5.2 times with the same level of accuracy. The method can be used in applied tasks where the time of object identification is critical. The developed method can be applied to search for several objects of different classes. The effectiveness of the method can be increased by varying the values of its parameters and adapting to the characteristics of the data.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\NJAIKU54\Gorokhovatskyi 等 - 2023 - Search for Visual Objects by Request in the Form of a Cluster Representation for the Structural Imag.pdf}
}

@misc{huangVoxPoserComposable3D2023,
  title = {{{VoxPoser}}: {{Composable 3D Value Maps}} for {{Robotic Manipulation}} with {{Language Models}}},
  shorttitle = {{{VoxPoser}}},
  author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and {Fei-Fei}, Li},
  year = {2023},
  month = nov,
  number = {arXiv:2307.05973},
  eprint = {2307.05973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a largescale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at voxposer.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\CANL9GNR\Huang 等 - 2023 - VoxPoser Composable 3D Value Maps for Robotic Manipulation with Language Models.pdf}
}

@misc{huaREDEEndendObject2021,
  title = {{{REDE}}: {{End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}}},
  shorttitle = {{{REDE}}},
  author = {Hua, Weitong and Zhou, Zhongxiang and Wu, Jun and Huang, Huang and Wang, Yue and Xiong, Rong},
  year = {2021},
  month = feb,
  eprint = {2010.12807},
  primaryclass = {cs},
  doi = {10.1109/LRA.2021.3062304},
  urldate = {2024-11-01},
  abstract = {Object 6D pose estimation is a fundamental task in many applications. Conventional methods solve the task by detecting and matching the keypoints, then estimating the pose. Recent efforts bringing deep learning into the problem mainly overcome the vulnerability of conventional methods to environmental variation due to the hand-crafted feature design. However, these methods cannot achieve end-to-end learning and good interpretability at the same time. In this paper, we propose REDE, a novel end-to-end object pose estimator using RGB-D data, which utilizes network for keypoint regression, and a differentiable geometric pose estimator for pose error back-propagation. Besides, to achieve better robustness when outlier keypoint prediction occurs, we further propose a differentiable outliers elimination method that regresses the candidate result and the confidence simultaneously. Via confidence weighted aggregation of multiple candidates, we can reduce the effect from the outliers in the final estimation. Finally, following the conventional method, we apply a learnable refinement process to further improve the estimation. The experimental results on three benchmark datasets show that REDE slightly outperforms the state-of-the-art approaches and is more robust to object occlusion. Our code is available at https://github.com/HuaWeitong/REDE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\CQTLNTZQ\Hua 等 - 2021 - REDE End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination.pdf}
}

@article{jauhriRobotLearningMobile2022,
  title = {Robot {{Learning}} of {{Mobile Manipulation}} with {{Reachability Behavior Priors}}},
  author = {Jauhri, Snehal and Peters, Jan and Chalvatzaki, Georgia},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  eprint = {2203.04051},
  primaryclass = {cs},
  pages = {8399--8406},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3188109},
  urldate = {2024-09-25},
  abstract = {Mobile Manipulation (MM) systems are ideal candidates for taking up the role of personal assistants in unstructured real-world environments. MM requires effective coordination of the robot's embodiments for tasks that require both mobility and manipulation. Reinforcement Learning (RL) holds the promise of endowing robots with adaptive behaviors, but most methods require large amounts of data. In this work, we study the integration of robotic reachability priors in actor-critic RL methods for accelerating the learning of MM for reaching and fetching tasks. Namely, we consider the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target. We devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization. Next, we train a reachability prior using data from the operational robot workspace, inspired by classical methods. Subsequently, we derive Boosted HyRL (BHyRL), a novel actor-critic algorithm that benefits from modeling Q-functions as a sum of residuals. For every new task, we transfer our learned residuals and learn the component of the Q-function that is task-specific, hence, maintaining the task structure from prior behaviors. Moreover, we find that regularizing the target policy with a prior policy yields more expressive behaviors. We evaluate our method in simulation in reaching and fetching tasks of increasing difficulty, and show the superior performance of BHyRL against baseline methods. Finally, we zero-transfer our learned 6D fetching policy with BHyRL to our MM robot: TIAGo++. For more details, refer to our project site: https://irosalab.com/rlmmbp.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {TLDR: This work considers the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target, and devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization.},
  file = {C:\Users\guaoxiang\Zotero\storage\3FUWWX5B\Jauhri 等 - 2022 - Robot Learning of Mobile Manipulation with Reachability Behavior Priors.pdf}
}

@misc{jiangRTMPoseRealTimeMultiPerson2023,
  title = {{{RTMPose}}: {{Real-Time Multi-Person Pose Estimation}} Based on {{MMPose}}},
  shorttitle = {{{RTMPose}}},
  author = {Jiang, Tao and Lu, Peng and Zhang, Li and Ma, Ningsheng and Han, Rui and Lyu, Chengqi and Li, Yining and Chen, Kai},
  year = {2023},
  month = jul,
  number = {arXiv:2303.07399},
  eprint = {2303.07399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. To bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multiperson pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75.8\% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-x achieves 65.3\% AP on COCO-WholeBody. To further evaluate RTMPose's capability in critical real-time applications, we also report the performance after deploying on the mobile device. Our RTMPose-s model achieves 72.2\% AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing open-source libraries. Our code and models are available at https://github.com/openmmlab/mmpose/tree/main/projects/rtmpose.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\LPV75RBD\Jiang 等 - 2023 - RTMPose Real-Time Multi-Person Pose Estimation based on MMPose.pdf}
}

@inproceedings{kalashnikovScalableDeepReinforcement2018,
  title = {Scalable {{Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Robot Learning}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  year = {2018},
  month = oct,
  pages = {651--673},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-10-13},
  abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\C5D82LVY\Kalashnikov 等 - 2018 - Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.pdf}
}

@misc{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2023},
  month = may,
  number = {arXiv:2209.07753},
  eprint = {2209.07753},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\EMNPHLHU\Liang 等 - 2023 - Code as Policies Language Model Programs for Embodied Control.pdf}
}

@misc{liManipLLMEmbodiedMultimodal2023,
  title = {{{ManipLLM}}: {{Embodied Multimodal Large Language Model}} for {{Object-Centric Robotic Manipulation}}},
  shorttitle = {{{ManipLLM}}},
  author = {Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.16217},
  eprint = {2312.16217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\D2Y4PPCK\Li 等 - 2023 - ManipLLM Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation.pdf}
}

@misc{majumdarImprovingVisionLanguageNavigation2020,
  title = {Improving {{Vision-and-Language Navigation}} with {{Image-Text Pairs}} from the {{Web}}},
  author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = may,
  number = {arXiv:2004.14973},
  eprint = {2004.14973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Following a navigation instruction such as `Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g.`stairs') to visual content in the environment (pixels corresponding to `stairs').},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\IAK6CMND\2004.14973v2.pdf}
}

@book{MeisheldonrossZhu;ZhaoXuanMinDengYi.GaiLuLunJiChuJiaoCheng2006,
  title = {{概率论基础教程}},
  author = {{(美)Sheldon Ross著 ; 赵选民等译.} and {罗斯.} and {赵选民.}},
  year = {2006},
  publisher = {机械工业出版社},
  address = {Bei jing},
  abstract = {本书系统介绍了概率论的基础理论及应用, 主要内容包括:组合分析, 概率论的公理, 条件概率与独立性, 随机变量及其分布, 数学期望, 极限定理, 随机模拟等},
  isbn = {978-7-111-18378-5},
  langid = {chinese},
  annotation = {OCLC: 303906169},
  file = {C:\Users\guaoxiang\Zotero\storage\I38BHNH5\(美)Sheldon Ross著 ; 赵选民等译. 等 - 2006 - 概率论基础教程.pdf}
}

@misc{PDFREDEEndEnd,
  title = {[{{PDF}}] {{REDE}}: {{End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}} {\textbar} {{Semantic Scholar}}},
  urldate = {2024-11-01},
  howpublished = {https://www.semanticscholar.org/paper/REDE\%3A-End-to-End-Object-6D-Pose-Robust-Estimation-Hua-Zhou/20f751603254a524c9b2de6c7076474e2260b945},
  keywords = {Pose},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\TQU6QK9R\\[PDF] REDE End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination .pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\WSPJGHV3\\20f751603254a524c9b2de6c7076474e2260b945.html}
}

@article{petersenHttpMatrixcookbookcom,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\TNN8BLHN\Petersen和Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@misc{puigVirtualHomeSimulatingHousehold2018,
  title = {{{VirtualHome}}: {{Simulating Household Activities}} via {{Programs}}},
  shorttitle = {{{VirtualHome}}},
  author = {Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  year = {2018},
  month = jun,
  number = {arXiv:1806.07011},
  eprint = {1806.07011},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to ``drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\KW24T4NN\Puig 等 - 2018 - VirtualHome Simulating Household Activities via Programs.pdf}
}

@misc{shahLMNavRoboticNavigation2022,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04429},
  eprint = {2207.04429},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\FRXT5V7X\Shah 等 - 2022 - LM-Nav Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.pdf}
}

@article{shakarjiLeastsquaresFittingAlgorithms1998,
  title = {Least-Squares Fitting Algorithms of the {{NIST}} Algorithm Testing System},
  author = {Shakarji, C.M.},
  year = {1998},
  month = nov,
  journal = {Journal of Research of the National Institute of Standards and Technology},
  volume = {103},
  number = {6},
  pages = {633},
  issn = {1044677X},
  doi = {10.6028/jres.103.043},
  urldate = {2024-11-08},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ME9FHGNG\Shakarji - 1998 - Least-squares fitting algorithms of the NIST algorithm testing system.pdf}
}

@article{strangArtLinearAlgebra2024,
  title = {{The Art of Linear Algebra}},
  author = {Strang, Gilbert and Hiranabe, Kenji and Fernandes, Ashley},
  year = {2024},
  month = mar,
  journal = {PRIMUS},
  pages = {1--14},
  issn = {1051-1970, 1935-4053},
  doi = {10.1080/10511970.2024.2321349},
  urldate = {2024-11-09},
  abstract = {我尝试为 Gilbert Strang 在书籍``Linear Algebra for Everyone''中介绍的矩阵的重要概念进行可视化图 释, 以促进从矩阵分解的角度对向量、矩阵计算和算法的理解. 1 它们包括矩阵分解 (Column-Row, CR)、高 斯消去法 (Gaussian Elimination, LU )、格拉姆-施密特正交化 (Gram-Schmidt Orthogonalization, QR)、特 征值和对角化 (Eigenvalues and Diagonalization, Q{$\Lambda$}QT)、和奇异值分解 (Singular Value Decomposition, U {$\Sigma$}V T).},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\2TSUQVIV\Strang 等 - 2024 - The Art of Linear Algebra.pdf}
}

@article{strangLinearAlgebraIts,
  title = {Linear {{Algebra}} and {{Its Applications}}},
  author = {Strang, Gilbert},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\667AGVU5\Strang - Linear Algebra and Its Applications.pdf}
}

@article{SymPyDocumentation,
  title = {{{SymPy Documentation}}},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\752UU7SA\SymPy Documentation.pdf}
}

@misc{tianRoboKeyGenRobotPose2024,
  title = {{{RoboKeyGen}}: {{Robot Pose}} and {{Joint Angles Estimation}} via {{Diffusion-based 3D Keypoint Generation}}},
  shorttitle = {{{RoboKeyGen}}},
  author = {Tian, Yang and Zhang, Jiyao and Huang, Guowei and Wang, Bin and Wang, Ping and Pang, Jiangmiao and Dong, Hao},
  year = {2024},
  month = mar,
  number = {arXiv:2403.18259},
  eprint = {2403.18259},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render\&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render\&compare method and achieves higher inference speed. Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\QMVHIEZ5\Tian 等 - 2024 - RoboKeyGen Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation.pdf}
}

@misc{touraniVisualSLAMWhat2022,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10491},
  eprint = {2210.10491},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of forty-five impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\J4ZSVYR4\\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\NCI8ZZCC\\2210.html}
}

@article{touraniVisualSLAMWhat2022a,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = nov,
  journal = {Sensors},
  volume = {22},
  number = {23},
  eprint = {2210.10491},
  primaryclass = {cs},
  pages = {9297},
  issn = {1424-8220},
  doi = {10.3390/s22239297},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of fortyfive impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\K574BRR3\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf}
}

@misc{TutorialGraphBasedSLAM,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-10-30},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\Z5EKTX2J\5681215.html}
}

@misc{TutorialGraphBasedSLAMa,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}}},
  urldate = {2024-10-30},
  abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based SLAM problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the SLAM problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  langid = {american},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\GQ3HB9FH\A Tutorial on Graph-Based SLAM.pdf}
}

@misc{vempralaChatGPTRoboticsDesign2023,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\G4XHY63Q\\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\6M95KQZT\\2306.html}
}

@misc{vempralaChatGPTRoboticsDesign2023a,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\9JWJV8QA\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf}
}

@article{wangYOLOv8PoseBoostAdvancementsMultimodal2024,
  title = {{{YOLOv8-PoseBoost}}: {{Advancements}} in {{Multimodal Robot Pose Keypoint Detection}}},
  shorttitle = {{{YOLOv8-PoseBoost}}},
  author = {Wang, Feng and Wang, Gang and Lu, Baoli},
  year = {2024},
  month = mar,
  journal = {Electronics},
  volume = {13},
  number = {6},
  pages = {1046},
  issn = {2079-9292},
  doi = {10.3390/electronics13061046},
  urldate = {2024-10-21},
  abstract = {In the field of multimodal robotics, achieving comprehensive and accurate perception of the surrounding environment is a highly sought-after objective. However, current methods still have limitations in motion keypoint detection, especially in scenarios involving small target detection and complex scenes. To address these challenges, we propose an innovative approach known as YOLOv8PoseBoost. This method introduces the Channel Attention Module (CBAM) to enhance the network's focus on small targets, thereby increasing sensitivity to small target individuals. Additionally, we employ multiple scale detection heads, enabling the algorithm to comprehensively detect individuals of varying sizes in images. The incorporation of cross-level connectivity channels further enhances the fusion of features between shallow and deep networks, reducing the rate of missed detections for small target individuals. We also introduce a Scale Invariant Intersection over Union (SIoU) redefined bounding box regression localization loss function, which accelerates model training convergence and improves detection accuracy. Through a series of experiments, we validate YOLOv8-PoseBoost's outstanding performance in motion keypoint detection for small targets and complex scenes. This innovative approach provides an effective solution for enhancing the perception and execution capabilities of multimodal robots. It has the potential to drive the development of multimodal robots across various application domains, holding both theoretical and practical significance.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\8JAC355V\Wang 等 - 2024 - YOLOv8-PoseBoost Advancements in Multimodal Robot Pose Keypoint Detection.pdf}
}

@misc{xuFASTLIOFastRobust2021,
  title = {{{FAST-LIO}}: {{A Fast}}, {{Robust LiDAR-inertial Odometry Package}} by {{Tightly-Coupled Iterated Kalman Filter}}},
  shorttitle = {{{FAST-LIO}}},
  author = {Xu, Wei and Zhang, Fu},
  year = {2021},
  month = apr,
  number = {arXiv:2010.08196},
  eprint = {2010.08196},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {This paper presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in real-time: running on a quadrotor onboard computer, it fuses more than 1,200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are open-sourced on Github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3UCVV3L8\\Xu和Zhang - 2021 - FAST-LIO A Fast, Robust LiDAR-inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\MR2J7BLA\\2010.html}
}

@misc{xuPIDNetRealtimeSemantic2023,
  title = {{{PIDNet}}: {{A Real-time Semantic Segmentation Network Inspired}} by {{PID Controllers}}},
  shorttitle = {{{PIDNet}}},
  author = {Xu, Jiacong and Xiong, Zixiang and Bhattacharyya, Shankar P.},
  year = {2023},
  month = apr,
  number = {arXiv:2206.02066},
  eprint = {2206.02066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-IntegralDerivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel threebranch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6\% mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1\% mIOU with speed of 153.7 FPS on CamVid.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\VCSCPDXZ\Xu 等 - 2023 - PIDNet A Real-time Semantic Segmentation Network Inspired by PID Controllers.pdf}
}

@book{YangJuZhenLun2003,
  title = {矩阵论},
  author = {杨, 明 and 刘, 先忠},
  year = {2003},
  edition = {第二版},
  publisher = {华中科技大学出版社},
  langid = {american},
  file = {C:\Users\guaoxiang\Zotero\storage\ITXWM2XQ\《矩阵论（第二版）》【杨明】.pdf}
}

@misc{zhangGAMMAGraspabilityAwareMobile2024,
  title = {{GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion}},
  shorttitle = {{GAMMA}},
  author = {Zhang, Jiazhao and Gireesh, Nandiraju and Wang, Jilong and Fang, Xiaomeng and Xu, Chaoyi and Chen, Weiguang and Dai, Liu and Wang, He},
  year = {2024},
  month = mar,
  number = {arXiv:2309.15459},
  eprint = {2309.15459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.15459},
  urldate = {2024-09-25},
  abstract = {Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses.},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\LQWSWFIX\\Zhang 等 - 2024 - GAMMA Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\9DUDT2VI\\2309.html}
}

@article{zhanImprovingEyeDisplay2019,
  title = {Improving Near-Eye Display Resolution by Polarization Multiplexing},
  author = {Zhan, Tao and Xiong, Jianghao and Tan, Guanjun and Lee, Yun-Han and Yang, Jilin and Liu, Sheng and Wu, Shin-Tson},
  year = {2019},
  month = may,
  journal = {Optics Express},
  volume = {27},
  number = {11},
  pages = {15327},
  issn = {1094-4087},
  doi = {10.1364/OE.27.015327},
  urldate = {2024-10-14},
  abstract = {We present here an optical approach to boost the apparent pixel density by utilizing the superimposition of two shifted-pixel grids generated by a Pancharatnam-Berry deflector (PBD). The content of the two shifted pixel grids are presented to the observer's eye simultaneously using a polarization-multiplexing method. Considering the compact and lightweight nature of PBD, this approach has potential applications in near-eye display systems. Moreover, the same concept can be extended to projection displays with proper modifications.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\BG8WLZ9Q\Zhan 等 - 2019 - Improving near-eye display resolution by polarization multiplexing.pdf}
}

@misc{zhaoAgentCerebrumController2023,
  title = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}: {{Implementing}} an {{Embodied LMM-based Agent}} on {{Drones}}},
  shorttitle = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}},
  author = {Zhao, Haoran and Pan, Fengxing and Ping, Huqiuyue and Zhou, Yaoming},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15033},
  eprint = {2311.15033},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this study, we present a novel paradigm for industrial robotic embodied agents, encapsulating an agent as cerebrum, controller as cerebellum architecture. Our approach harnesses the power of Large Multimodal Models (LMMs) within an agent framework known as AeroAgent, tailored for drone technology in industrial settings. To facilitate seamless integration with robotic systems, we introduce ROSchain, a bespoke linkage framework connecting LMM-based agents to the Robot Operating System (ROS). We report findings from extensive empirical research, including simulated experiments on the Airgen and real-world case study, particularly in individual search and rescue operations. The results demonstrate AeroAgent's superior performance in comparison to existing Deep Reinforcement Learning (DRL)-based agents, highlighting the advantages of the embodied LMM in complex, real-world scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\54QUDRMU\Zhao 等 - 2023 - Agent as Cerebrum, Controller as Cerebellum Implementing an Embodied LMM-based Agent on Drones.pdf}
}

@inproceedings{zhengyouzhangFlexibleCameraCalibration1999,
  title = {Flexible Camera Calibration by Viewing a Plane from Unknown Orientations},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {{Zhengyou Zhang}},
  year = {1999},
  pages = {666-673 vol.1},
  publisher = {IEEE},
  address = {Kerkyra, Greece},
  doi = {10.1109/ICCV.1999.791289},
  urldate = {2024-11-11},
  abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use. The corresponding software is available from the author's Web page.},
  isbn = {978-0-7695-0164-2},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\XECWGQE4\Zhengyou Zhang - 1999 - Flexible camera calibration by viewing a plane from unknown orientations.pdf}
}
